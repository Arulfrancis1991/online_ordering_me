import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler

from sklearn import decomposition
from sklearn import datasets

import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

import sys
sys.path.insert(0, '/etc/jumbo')


from  requests.auth  import  HTTPBasicAuth
from  pyhive  import  presto


def  rdash_query(query):
        conn  =  presto.connect(
                host="datascience.presto.zomato.farm",
                port=80,
                username="arul.francis@zomato.com",
                catalog='hive',
                schema='jumbo2',
                requests_kwargs={'auth':  HTTPBasicAuth("presto_zds",  "KfcjW5KkcvkGhwJZLBRSzJRHc34BLnZk")} # user_id and password
        )

        df=  pd.read_sql(query,conn)
        conn.close()
        return  df
    
    
from datetime import datetime
from datetime import timedelta
import datetime

if len(sys.argv) == 2:
    end_time = sys.argv[1]
else:
    end_time= datetime.datetime.strftime(datetime.datetime.now() - timedelta(days=5), '%Y%m%d')
    
start_time = datetime.datetime.strftime(datetime.datetime.strptime(end_time, '%Y%m%d') - timedelta(days=60), "%Y%m%d")


start_time
end_time




########get transaction data for 60 day active repeat users
raw_data = rdash_query("""
    select 
    zoh.user_id, cc.city_id,
    
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT <= 20 then zoh.tab_id end) order_aov20,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 20 and zoh.MERCHANT_RECEIVED_AMOUNT <= 30 then zoh.tab_id end) order_aov30,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 30 and zoh.MERCHANT_RECEIVED_AMOUNT <= 40 then zoh.tab_id end) order_aov40,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 40 and zoh.MERCHANT_RECEIVED_AMOUNT <= 50 then zoh.tab_id end) order_aov50,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 50 and zoh.MERCHANT_RECEIVED_AMOUNT <= 60 then zoh.tab_id end) order_aov60,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 60 and zoh.MERCHANT_RECEIVED_AMOUNT <= 70 then zoh.tab_id end) order_aov70,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 70 and zoh.MERCHANT_RECEIVED_AMOUNT <= 80 then zoh.tab_id end) order_aov80,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 80 and zoh.MERCHANT_RECEIVED_AMOUNT <= 90 then zoh.tab_id end) order_aov90,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 90 and zoh.MERCHANT_RECEIVED_AMOUNT <= 100 then zoh.tab_id end) order_aov100,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 100 and zoh.MERCHANT_RECEIVED_AMOUNT <= 110 then zoh.tab_id end) order_aov110,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 110 and zoh.MERCHANT_RECEIVED_AMOUNT <= 120 then zoh.tab_id end) order_aov120,
    count(distinct case when zoh.MERCHANT_RECEIVED_AMOUNT > 120 then zoh.tab_id end) order_gt_aov120,
    count(distinct zoh.tab_id) as orders, 

    avg(zoh.MERCHANT_RECEIVED_AMOUNT) as aov,
    sum(zoh.bill_subtotal) / sum(cast(json_extract_scalar(cast(items as json),'$.item_quantity') as bigint)) as per_unit_cost,
    count(case when zoh.delivery_charge > 0 and  zoh.packaging_charges > 0 then zoh.tab_id end) as orders_with_extra_charge,
    avg(case when case when zoh.delivery_charge IS NOT NULL then zoh.delivery_charge else 0 end  + case when zoh.packaging_charges IS NOT NULL then zoh.packaging_charges else 0 end > 0 then case when zoh.delivery_charge IS NOT NULL then zoh.delivery_charge else 0 end  + case when zoh.packaging_charges IS NOT NULL then zoh.packaging_charges else 0 end end) as avg_extra_charge,
    count(case when zoh.voucher_discount >0 then zoh.tab_id end) as promo_discounted_order,
    
    avg((
      coalesce(zoh.salt_discount,0) + 
      coalesce(zoh.pepper_discount,0) + 
      coalesce(zoh.voucher_discount,0) + 
      coalesce(zoh.zomato_voucher_discount,0) + 
      coalesce(zoh.merchant_voucher_discount,0)
      )) as avg_discounted_value,
    
    sum((
      coalesce(zoh.salt_discount,0) + 
      coalesce(zoh.pepper_discount,0) + 
      coalesce(zoh.voucher_discount,0) + 
      coalesce(zoh.zomato_voucher_discount,0) + 
      coalesce(zoh.merchant_voucher_discount,0)
      )) as total_discounted_value,
    
    avg((
      coalesce(zoh.salt_discount,0) + 
      coalesce(zoh.pepper_discount,0) + 
      coalesce(zoh.voucher_discount,0) + 
      coalesce(zoh.zomato_voucher_discount,0) + 
      coalesce(zoh.merchant_voucher_discount,0)
      )/((
        coalesce(zoh.salt_discount,0) + 
        coalesce(zoh.pepper_discount,0) + 
        coalesce(zoh.voucher_discount,0) + 
        coalesce(zoh.zomato_voucher_discount,0) + 
        coalesce(zoh.merchant_voucher_discount,0)
        ) + 
      coalesce(zoh.MERCHANT_RECEIVED_AMOUNT,0)
      )) as discounted_share,
    
    count(mtm.tab_id) as qvm_orders,
    
    round(avg(zoh.MERCHANT_RECEIVED_AMOUNT / ( 0.4 * r.cost_for_two)),2) as order_size,
    count(distinct case when rt.rating <= 3 then zoh.tab_id end) as orders_res_lte_3,
    count(distinct case when rt.rating > 3 and rating <= 3.5 then zoh.tab_id end) as orders_res_lte_35,
    count(distinct case when rt.rating > 3.5 and rating <= 4 then zoh.tab_id end) as orders_res_lte_4,
    count(distinct case when rt.rating > 4 and rating <= 4.5 then zoh.tab_id end) as orders_res_lte_45,
    count(distinct case when rt.rating > 4.5 then zoh.tab_id end) as orders_res_gt_45
    
    from jumbo_derived.zomato_order_history as zoh
    cross join unnest(item_array) AS t(items)
    inner join zomato4.restaurants as r on r.res_id = cast(zoh.res_id as varchar)
    
    inner join 
    (
      select user_id, city_id
      from 
      (
        select user_id, city_id, orders, row_number() over (partition by user_id order by orders desc) as rnk
        from 
        (
          select user_id, city_id, count(distinct tab_id) as orders 
          from jumbo_derived.zomato_order_history as zoh
          where zoh.dt between '{start_time}' and '{end_time}'
          and zoh.status IN (2,6,7,8)
          and zoh.country_id = 214
          group by 1,2
        ) group by 1,2,3
      ) where rnk = 1
    ) as cc on cc.user_id = zoh.user_id
    
    
    left join  
    (
      select 
      resid res_id, 
      avg(ratingnorm) as rating 
      from jumbo2.resrank 
      where dt between  '{start_time}' and '{end_time}' 
      AND resid in (select distinct cast(res_id as bigint) 
        from zomato4.restaurants r 
        inner join zomato4.cities c on r.city_id = c.city_id 
        where country_id IN (214)) group by 1
    ) as rt on zoh.res_id = rt.res_id

    
    
    where zoh.dt between '{start_time}' and '{end_time}'
    and zoh.status IN (2,6,7,8)
    and zoh.country_id = 214
    
    group by 1,2
     """.format(start_time=start_time,end_time=end_time))
 


raw_data.head()
cust_seg = raw_data.drop(['total_discounted_value'], axis = 1)

cust_seg['per_unit_cost'] = cust_seg['per_unit_cost'].astype(float)
user_data2 = cust_seg.copy()
user_data = cust_seg.copy()
user_data= user_data.drop(user_data.columns[user_data.columns.str.contains('unnamed',case = False)],axis = 1)
user_data= user_data.drop(['order_size'],axis = 1)
user_data = user_data.fillna(0)



np.percentile(cust_seg['orders'], 99)

list(cust_seg.columns)

# transform data into ratios of orders
user_data.iloc[:,2:14] =  user_data.iloc[:,2:14].div(user_data.orders, axis=0)
user_data['orders_with_extra_charge'] = user_data['orders_with_extra_charge'].div(user_data.orders, axis=0)
user_data['promo_discounted_order'] =   user_data['promo_discounted_order'].div(user_data.orders, axis=0)
user_data.iloc[:,22:28] =   user_data.iloc[:,22:28].div(user_data.orders, axis=0)

## Replacing null values with 0
user_data = user_data.fillna(0)


## Outlier treatment
user_data['orders'] = np.where(user_data['orders'] >100, 100, user_data['orders'])
user_data['avg_discounted_value'] = np.where(user_data['avg_discounted_value'] >np.percentile(user_data['avg_discounted_value'], 99), np.percentile(user_data['avg_discounted_value'], 99), user_data['avg_discounted_value'])
user_data['aov'] = np.where(user_data['aov'] >np.percentile(user_data['aov'], 99.99), np.percentile(user_data['aov'], 99.99), user_data['aov'])
user_data['per_unit_cost'] = np.where(user_data['per_unit_cost'] >np.percentile(user_data['per_unit_cost'], 99.99), np.percentile(user_data['per_unit_cost'], 99.99), user_data['per_unit_cost'])
user_data['avg_extra_charge'] = np.where(user_data['avg_extra_charge'] >np.percentile(user_data['avg_extra_charge'], 99.99), np.percentile(user_data['avg_extra_charge'], 99.99), user_data['avg_extra_charge'])




(100 - user_data.orders.mean())/user_data.orders.std()


list(user_data.columns)


user_data = user_data.drop(['orders'],axis = 1)

user_data[user_data.city_id == 51] 


df = user_data[user_data.city_id == 51].copy()
df.iloc[:, 2:]
df = df.drop(['qvm_orders'], axis = 1)
df.head()


#df.isna().values.any()


"""
#############################
# (x-mean)/std
x = pd.DataFrame()
for j in list(df.city_id.unique()):
    df1 = df[df.city_id == j]
    for i in list(df1.iloc[:, 2:28][df1.city_id == j].columns):        
        colname = i
        m = df1[i].mean()
        s = df1[i].std()
        df1[colname] = (df1[i] - m)/s

    x = x.append(df1)
    print(j)

#############################
"""

"""
#########################
# Standarize values
from sklearn.preprocessing import StandardScaler
final = pd.DataFrame()
for i in list(df.city_id.unique()): 
    df_city = df[df.city_id==i] 
    df_Std = StandardScaler().fit_transform(df_city.iloc[:,2:])
    final = pd.concat([final, pd.DataFrame(df_Std)], axis=0)
    print(i)
############################
"""

from sklearn.preprocessing import StandardScaler
final = pd.DataFrame()
final = pd.DataFrame(StandardScaler().fit_transform(df.iloc[:,2:]))



final.isna().values.any()


# applying WCSS elboy method
from sklearn.cluster import KMeans
wcss = []
for i in range(1,11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(final)
    wcss.append(kmeans.inertia_)
    
plt.plot(range(1,11), wcss)
plt.title('The Elbow Method')
plt.xlabel('# of Clusters')
plt.ylabel('WCSS')
plt.show()


user_data2 = user_data[user_data.city_id == 51]

# KMeans
from sklearn.preprocessing import StandardScaler
kmeans = KMeans(n_clusters= 5, init= 'k-means++', max_iter=300, n_init= 10, random_state=0)
user_data2['cluster'] = kmeans.fit_predict(final)





user_data3 = pd.merge(cust_seg[cust_seg.city_id == 51], user_data2[['user_id','cluster']], left_on = 'user_id', right_on = 'user_id', how = 'left')




#df = pd.concat([df, x['cluster']], axis=1)


summary_data= user_data3.groupby(['cluster'], as_index=False).agg({
        'user_id':{'No Of Users':'count'},
        'order_aov20':{'order_aov20':'sum'},
		'order_aov30':{'order_aov30':'sum'},
		'order_aov40':{'order_aov40':'sum'},
		'order_aov50':{'order_aov50':'sum'},
		'order_aov60':{'order_aov60':'sum'},
		'order_aov70':{'order_aov70':'sum'},
		'order_aov80':{'order_aov80':'sum'},
		'order_aov90':{'order_aov90':'sum'},
		'order_aov100':{'order_aov100':'sum'},
		'order_aov110':{'order_aov110':'sum'},
		'order_aov120':{'order_aov120':'sum'},
        'order_gt_aov120':{'order_gt_aov120':'sum'},
		'orders':{'No of Orders In last 60 Days':'sum'},
        'aov':{'AOV':'mean'},
		'per_unit_cost':{'Per Unit Cost': 'mean'},
        'orders_with_extra_charge':{'Orders with Extra Charges':'sum'},
        'avg_extra_charge':{'Average Extra Charges':'mean'},
        'promo_discounted_order':{'Total promo Discont Orders':'sum'},
        'avg_discounted_value':{'Average Voucher Discount':'mean'},
        'discounted_share':{'discounted_share':'sum'},
        'qvm_orders':{'qvm_orders':'sum'},
        'orders_res_lte_3':{'orders_res_lte_3':'sum'},
        'orders_res_lte_35':{'orders_res_lte_35':'sum'},
        'orders_res_lte_4':{'orders_res_lte_4':'sum'},
        'orders_res_lte_45':{'orders_res_lte_45':'sum'},
        'orders_res_gt_45':{'orders_res_gt_45':'sum'} })
        
summary_data.columns = summary_data.columns.droplevel(0)

list(summary_data.columns)

list(user_data3.columns)


user_data3.groupby(['cluster'], as_index=False).agg({
        'avg_extra_charge':'mean',
        'avg_discounted_value':'mean'})


user_data3.groupby(['cluster'], as_index=False).agg({
    'avg_discounted_value':'sum'})


#del user_data4 
user_data4 = pd.merge(raw_data[raw_data.city_id == 51], 
                      user_data3[['user_id','cluster']], 
                      left_on = 'user_id', 
                      right_on = 'user_id', 
                      how = 'left')

list(user_data4.columns)
user_data4['total_discounted_value'] = user_data4['total_discounted_value'].astype(int)
user_data4.groupby(['cluster'], as_index = False).agg({
        'total_discounted_value':'sum'})
